# LoRA 系列微调方法对比笔记

LoRA、QLoRA、AdaLoRA 和 LoRA+ 都是 LoRA（Low-Rank Adaptation）技术的变种，它们通过不同的优化方式进一步提升了 LoRA 微调的效率、性能和灵活性。它们在实现目标、适用场景以及技术细节上有所不同。下面是它们的对比：

## 1. LoRA (Low-Rank Adaptation)

LoRA 是一种高效的微调方法，特别适用于大规模预训练模型。它的基本原理是在神经网络的线性层中引入低秩矩阵，来减少需要训练的参数量，从而减少训练开销和显存占用。

### LoRA 特点：

- **基本思想：** 通过添加低秩矩阵（通常是一个小的矩阵）到模型的权重矩阵中，从而在保持原始预训练模型大部分权重不变的情况下，进行有效的微调。
- **显存占用低：** 相比于直接微调全参数，LoRA 只需要更新低秩矩阵，因此显存消耗较小，适合资源有限的环境。
- **效率高：** 由于微调的参数较少，训练速度通常较快。

### 优缺点：

- **优点：** 计算开销小，显存消耗少，适合大模型微调。
- **缺点：** 在某些任务上，可能不如全量微调的模型表现优异，特别是在需要大量调整模型参数的情况下。

## 2. QLoRA (Quantized Low-Rank Adaptation)

### QLoRA 的定义与原理 ： 

​	QLoRA 在 LoRA 的基础上引入了量化技术，通过将预训练模型的权重量化为 4-bit 精度来减少内存占用，同时保留 LoRA 的低秩更新机制来实现参数高效微调。QLoRA 的核心思想是将模型的计算分为两部分：量化的预训练权重（冻结）和低秩更新矩阵（可训练），从而大幅降低微调所需的 GPU 内存，同时保持任务适配的灵活性。

工作原理：

**1.4-bit 量化：**  

​	预训练模型的权重矩阵 $ W \in \mathbb{R}^{d \times k} $ 被量化为 4-bit 精度（如使用 NF4，NormalFloat4 格式），显著减少内存占用。 量化后的权重存储在低精度格式，但在计算时通过动态反量化（dequantization）恢复到较高精度（如 16-bit）以进行前向和反向传播。量化技术基于 Block-Wise Quantization，将权重矩阵分块量化以优化精度和效率。  

**2.LoRA 低秩更新：** 
	与 LoRA 相同，QLoRA 在权重矩阵上添加低秩更新 $ \Delta W = A \cdot B $，其中 $ A \in \mathbb{R}^{d \times r} $，$ B \in \mathbb{R}^{r \times k} $，$ r \ll \min(d, k) $。  仅训练 $ A $ 和 $ B $ 的参数，冻结量化的原始权重 $ W $。 
前向传播计算为：
$$
h = (W_{\text{quantized}} + \Delta W)x = W_{\text{quantized}}x + (A \cdot B)x
$$
其中，$ W_{\text{quantized}} $ 是量化的权重，$ x $ 是输入，$ h $ 是输出。

**3.双重量化（Double Quantization）：** 
	QLoRA 引入了双重量化技术，进一步优化内存使用。 第一层量化将权重量化为 4-bit，第二层量化将量化的缩放因子（scaling factors）也量化为较低精度（如 8-bit），减少存储开销。  

**4.分页优化（Paged Optimizers）：** 
	QLoRA 使用 NVIDIA 的统一内存技术，将优化器状态（如 Adam 的动量）分页到 CPU 内存，降低 GPU 内存需求。  

**训练：** 
冻结量化的预训练权重，仅优化 LoRA 矩阵 $ A $ 和 $ B $。  
训练时，量化权重在计算前动态反量化为 16-bit 以保证精度。  

**推理：** 
推理时可以将 LoRA 更新 $ \Delta W = A \cdot B $ 合并到量化的权重 $ W_{\text{quantized}} $，得到更新后的权重 $ W' $，无需额外计算开销。  
或者直接使用量化的 $ W_{\text{quantized}} $ 和 LoRA 模块进行推理。  

**参数效率：**  

- 4-bit 量化将权重内存占用减少约 4 倍（相比 16-bit 浮点数）。  
- LoRA 的低秩更新仅需训练极少参数（通常占总参数的 0.01%-1%）。  
- 例如，一个 70B 参数模型在 16-bit 精度下需要约 140 GB 内存，4-bit 量化后降至 ~35 GB，结合 LoRA 可在单 24 GB GPU 上微调。

### QLoRA 特点：

- **量化：** QLoRA 不仅在模型中引入低秩矩阵，同时还将低秩矩阵进行量化，从而显著降低存储和计算成本。
- **内存和显存优化：** 量化后的低秩矩阵使用更少的内存，显著减少微调时的显存占用，使得微调过程能在更小的显存环境中运行。
- **提高训练效率：** 量化可以加速训练过程，尤其是对于大规模模型，能够减少训练所需的硬件资源。

### 优缺点：

- **优点：** 比传统的 LoRA 显著减少内存占用，尤其适用于显存较小的环境。
- **缺点：** 量化可能会导致部分精度损失，尤其是在需要非常高精度的任务上，量化后的模型可能无法完全保留原有模型的性能。

## 3. AdaLoRA (Adaptive Low-Rank Adaptation)

AdaLoRA 在 LoRA 的基础上引入了自适应机制，能够根据任务和数据的特点动态调整低秩矩阵的秩。传统的 LoRA 需要人为设定秩的大小，而 AdaLoRA 通过动态调整，使得秩的选择更加灵活，并且能够适应不同的训练需求。

### AdaLoRA 特点：

- **自适应调整秩：** AdaLoRA 会根据模型的训练进度和数据的复杂度动态地调整低秩矩阵的秩。这样能够确保模型在训练过程中能够有效学习到必要的特征，同时避免过拟合或欠拟合。
- **灵活性更强：** 由于动态调整秩，AdaLoRA 在不同任务和数据集上的适应能力更强，能够更好地适应复杂的学习任务。
- **效率与效果平衡：** 通过动态调整秩，AdaLoRA 在训练速度和模型性能之间达到了较好的平衡。

### 优缺点：

- **优点：** 自适应调整使得模型能在不同任务和数据集上表现出更好的性能，避免了固定秩带来的限制。
- **缺点：** 相较于传统 LoRA，AdaLoRA 在训练过程中需要更多的计算和调试，可能会增加训练的复杂度。

## 4. LoRA+ (LoRA Plus)

LoRA+ 是 LoRA 的一种增强版本，旨在进一步提高微调过程的效率和效果。LoRA+ 引入了多种技术，如更精细的优化方法、更强的正则化策略，以及更有效的模型融合技术。

### LoRA+ 特点：

- **增强优化方法：** LoRA+ 在 LoRA 基础上加入了更多先进的优化算法，例如自适应学习率、正则化策略等，从而提高模型的收敛速度和精度。
- **模型融合：** LoRA+ 有时会结合多个预训练模型，通过模型融合技术进一步提高性能。
- **复杂任务适应性强：** LoRA+ 可以适应更为复杂的任务，尤其是在多任务学习或处理复杂数据集时。

### 优缺点：

- **优点：** 在提升微调效率的同时，还能更好地应对复杂任务，适合需要多任务处理的场景。
- **缺点：** 与 LoRA 相比，LoRA+ 可能需要更多的计算资源和时间，适合显存和计算资源充足的环境。

## 总结对比表

| 特性         | LoRA                   | QLoRA                      | AdaLoRA                              | LoRA+                                |
| ------------ | ---------------------- | -------------------------- | ------------------------------------ | ------------------------------------ |
| **核心思想** | 低秩矩阵微调           | LoRA + 量化                | 自适应调整低秩矩阵秩                 | LoRA增强版，加入更多优化策略和正则化 |
| **存储优化** | 显存占用少             | 通过量化进一步降低显存占用 | 动态调整秩以优化显存和计算           | 改进优化方法，适用于更复杂任务       |
| **训练效率** | 高效微调，减少训练参数 | 量化加速训练，减少显存占用 | 动态调整提高训练效果，平衡效率与效果 | 增强优化加速训练，适合多任务学习     |
| **适用场景** | 适用于大模型微调       | 显存较小的设备，低显存需求 | 复杂任务和数据集，灵活调整模型结构   | 多任务学习、复杂任务，优化多种任务   |
| **缺点**     | 固定秩不灵活           | 量化可能带来精度损失       | 训练过程复杂，需要更多的调试         | 需要更多计算资源，训练时间较长       |

## 选择建议：

- **LoRA：** 适合在资源有限的情况下对大规模预训练模型进行微调，计算开销小，显存占用少。
- **QLoRA：** 适合显存较小的环境，尤其是在训练大模型时，量化能够显著降低内存需求。
- **AdaLoRA：** 适合需要自适应调整模型秩的复杂任务，能够根据训练进度和数据调整秩的大小，适应性更强。
- **LoRA+：** 适合多任务学习和复杂的任务场景，能够有效提升训练效果，但需要更多的计算资源和时间。

